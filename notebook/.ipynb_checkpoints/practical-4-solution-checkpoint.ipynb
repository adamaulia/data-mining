{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03bf8c7-5ba3-4a08-b344-1064ab12b7be",
   "metadata": {},
   "source": [
    "## Practical 4\n",
    "\n",
    "This practical will mostly focus on preparing the data to build a classification model and assessing what the classifier learned. Unlike the previous practicals, there will not be tasks leading to questions for you to answer. Instead, the practical will provide a step-by-step solution on how to build and asses a classifier.\n",
    "\n",
    "*Important note:* Please ask if you cannot answer a task during the practicals fully or feel unsure about your answer (even after the explanation). You must develop the correct intuitions for each of the points we discuss here. Sometimes they do not ‘click’ by themselves; they require repeated practice and interpretation, and not every explanation works for everyone. We will be very happy to answer all your questions on Canvas!\n",
    "\n",
    "### Setting up for Predictions\n",
    "\n",
    "When acquiring data yourself, there is always a clear task or goal. However, when working with real-world data, it is often your goal to discover purpose in data. Moreover, it might also be the case that you already have a particular goal in mind but are still able to find surprising patterns with some thorough analysis. Collection, cleaning, and general preparation of data cover the majority of the effort in Data Science research.\n",
    "\n",
    "Learning about common issues in real-world data and the ways to solve them takes practice and patience. Even when dealing with toy datasets, as you have already seen, the combination of your data and tools can pose quite the obstruction on the way to your final goal: doing predictions.\n",
    "\n",
    "There is a manifold of different issues that arise in the process described above. We will cover the following in this practical:\n",
    "- Getting your data in the correct data types.\n",
    "- Plotting to gain extra insight and identifying anomalies in data.\n",
    "- Preprocessing to improve the information in your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50860df-0f18-4700-8400-9d2864655fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>director_name</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>duration</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>gross</th>\n",
       "      <th>genres</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>budget</th>\n",
       "      <th>title_year</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James-Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel-David-Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Color</td>\n",
       "      <td>Gore-Verbinski</td>\n",
       "      <td>302.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Orlando-Bloom</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>309404152.0</td>\n",
       "      <td>Action|Adventure|Fantasy</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>300000000.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Color</td>\n",
       "      <td>Sam-Mendes</td>\n",
       "      <td>602.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>Rory-Kinnear</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>200074175.0</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>UK</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>245000000.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.35</td>\n",
       "      <td>85000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Color</td>\n",
       "      <td>Christopher-Nolan</td>\n",
       "      <td>813.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>Christian-Bale</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>448130642.0</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>250000000.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.35</td>\n",
       "      <td>164000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doug-Walker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rob-Walker</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   color      director_name  num_critic_for_reviews  duration  \\\n",
       "0  Color      James-Cameron                   723.0     178.0   \n",
       "1  Color     Gore-Verbinski                   302.0     169.0   \n",
       "2  Color         Sam-Mendes                   602.0     148.0   \n",
       "3  Color  Christopher-Nolan                   813.0     164.0   \n",
       "4    NaN        Doug-Walker                     NaN       NaN   \n",
       "\n",
       "   director_facebook_likes  actor_3_facebook_likes      actor_2_name  \\\n",
       "0                      0.0                   855.0  Joel-David-Moore   \n",
       "1                    563.0                  1000.0     Orlando-Bloom   \n",
       "2                      0.0                   161.0      Rory-Kinnear   \n",
       "3                  22000.0                 23000.0    Christian-Bale   \n",
       "4                    131.0                     NaN        Rob-Walker   \n",
       "\n",
       "   actor_1_facebook_likes        gross                           genres  ...  \\\n",
       "0                  1000.0  760505847.0  Action|Adventure|Fantasy|Sci-Fi  ...   \n",
       "1                 40000.0  309404152.0         Action|Adventure|Fantasy  ...   \n",
       "2                 11000.0  200074175.0        Action|Adventure|Thriller  ...   \n",
       "3                 27000.0  448130642.0                  Action|Thriller  ...   \n",
       "4                   131.0          NaN                      Documentary  ...   \n",
       "\n",
       "  language country  content_rating       budget title_year  \\\n",
       "0  English     USA           PG-13  237000000.0     2009.0   \n",
       "1  English     USA           PG-13  300000000.0     2007.0   \n",
       "2  English      UK           PG-13  245000000.0     2015.0   \n",
       "3  English     USA           PG-13  250000000.0     2012.0   \n",
       "4      NaN     NaN             NaN          NaN        NaN   \n",
       "\n",
       "   actor_2_facebook_likes imdb_score aspect_ratio  movie_facebook_likes  \\\n",
       "0                   936.0        7.9         1.78                 33000   \n",
       "1                  5000.0        7.1         2.35                     0   \n",
       "2                   393.0        6.8         2.35                 85000   \n",
       "3                 23000.0        8.5         2.35                164000   \n",
       "4                    12.0        7.1          NaN                     0   \n",
       "\n",
       "  quality  \n",
       "0       4  \n",
       "1       4  \n",
       "2       3  \n",
       "3       5  \n",
       "4       4  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/IMDB/imdb.csv', na_values='?')\n",
    "df.replace({\"quality\": {\"very-bad\": 1, \"bad\": 2, \"okay\": 3, \"good\": 4, \"very-good\": 5}}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640f94f-129f-4ae7-857b-cf001309ba31",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Now that we have looked at some interesting characteristics of this particular dataset, we will discuss a few methods to enrich it and fix some of the errors we found. Again, please keep in mind that there are many more things that can be done. We will show a selection of them.\n",
    "\n",
    "##### Task 1. Dealing with highly unique features \n",
    "\n",
    "As we have discussed in the Text-Mining-related video lectures, language is often represented as text vectors. However, what happens when we are dealing with actor names? These are not embedded in a piece of text, and we cannot split them into separate tokens (chances of the same name is an important feature are slim). However, it does contain some information, intuitively at least. When you see names such as Ryan Gosling, Dave Bautista, Jared Leto, and Harrison Ford on one poster, you could make an educated guess that there needs to be something very wrong with the movie for it to get a low IMDB score. So, we associate some quality with certain names. A quick way to convey this kind of world knowledge is replacing the actor names with the average IMDB scores for the movies they have starred in. This is quite a straightforward operation in ``pandas``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d94d09b-11e7-4715-b1f3-3c34908b5438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actor_1_name\n",
       "Krystyna-Janda        9.1\n",
       "Jack-Warden           8.9\n",
       "Rob-McElhenney        8.8\n",
       "Kimberley-Crossman    8.7\n",
       "Abigail-Evans         8.7\n",
       "Elina-Abai-Kyzy       8.7\n",
       "Jackie-Gleason        8.7\n",
       "Takashi-Shimura       8.7\n",
       "Maria-Pia-Calzone     8.7\n",
       "Ruth-Wilson           8.6\n",
       "Name: imdb_score, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('actor_1_name')['imdb_score'].mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d080b6-c859-4ebb-8a6e-8e47521d659e",
   "metadata": {},
   "source": [
    "However, the mean score might not be enough. We would also want to introduce some reliability scores. The way that is solved here is not the only one, but at least it considers (a) popularity is important (e.g., starring in many movies), and (b) the scores of those movies are important. Note that this metric can be offset by being old (see most of the listed actors below) AND popular. However, by using the sum over the z-scores of movies scores that an actor starred in, we are provided with some indication of how consistently good an actor is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d973c80-e170-4046-80fc-9c9aa4933d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actor_1_name\n",
       "Tom-Hanks                 20.967048\n",
       "Leonardo-DiCaprio         19.656199\n",
       "Harrison-Ford             17.215400\n",
       "Denzel-Washington         17.105197\n",
       "Christian-Bale            15.497207\n",
       "Matt-Damon                14.440725\n",
       "Philip-Seymour-Hoffman    14.186358\n",
       "Kevin-Spacey              13.846442\n",
       "Robert-De-Niro            13.298695\n",
       "Tom-Cruise                13.025713\n",
       "Name: imdb_z, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats.mstats import zscore\n",
    "\n",
    "df['imdb_z'] = zscore(df['imdb_score'])\n",
    "pd.Series(df.groupby('actor_1_name')['imdb_z'].sum()).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f6046-2f1a-485f-bfe0-ad96aae37627",
   "metadata": {},
   "source": [
    "Note that when we do not consider the number of movies, we get very different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655dc11b-5c21-4582-bc47-3ed79acf2ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actor_1_name\n",
       "Krystyna-Janda        2.361291\n",
       "Jack-Warden           2.183659\n",
       "Rob-McElhenney        2.094843\n",
       "Takashi-Shimura       2.006028\n",
       "Kimberley-Crossman    2.006028\n",
       "Jackie-Gleason        2.006028\n",
       "Abigail-Evans         2.006028\n",
       "Elina-Abai-Kyzy       2.006028\n",
       "Maria-Pia-Calzone     2.006028\n",
       "Bunta-Sugawara        1.917212\n",
       "Name: imdb_z, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(df.groupby('actor_1_name')['imdb_z'].mean()).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef74b0-c2c5-4dc1-b28b-9bf714f409da",
   "metadata": {},
   "source": [
    "##### Task 2. Dealing with text data\n",
    "\n",
    "What to do when we have text data? While we could set up a prediction task for the movie score based on the plot alone, this dataset does not provide the rich textual representation required for a proper computational linguistics approach. Instead, we will take the keywords from the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828f25d4-8804-4e2e-a0e0-d9ec8c61af69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               avatar|future|marine|native|paraplegic\n",
       "1    goddess|marriage-ceremony|marriage-proposal|pi...\n",
       "2                  bomb|espionage|sequel|spy|terrorist\n",
       "3    deception|imprisonment|lawlessness|police-offi...\n",
       "4                                                  NaN\n",
       "Name: plot_keywords, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['plot_keywords'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fb3ad-0939-45cb-9c17-2a2302f63793",
   "metadata": {},
   "source": [
    "We will have to do two things here: split the different keywords and then convert them into word counts. Scikit-learn offers a ``CountVectorizer`` to do just this. It detects the char | out of the box as a token boundary. We only have to make sure that the missing values (NaN) are replaced by an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a5c3d7-b56d-4a26-b279-f5dcab1af291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['plot_keywords'] = df['plot_keywords'].fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f719b7-b5ad-4790-9e6d-42d38bd0a585",
   "metadata": {},
   "source": [
    "Note that the transformation returns a sparse matrix. If, afterward, we want to view it in a ``DataFrame`` format again, it needs to be a dense matrix (or we have to do some special ``pandas`` loading procedure for sparse matrices). To restrict the words to frequent ones only, we can use ``max_features``. Afterward, you can join this word matrix to your original ``DataFrame`` if you prefer, or even do something fancier like using the ``TfidfVectorizer``. However, to keep further interpretation in this practical straightforward, we will not be doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ab10cf-8e64-48f1-9c96-552daa9566b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1950s</th>\n",
       "      <th>1960s</th>\n",
       "      <th>1970s</th>\n",
       "      <th>1980s</th>\n",
       "      <th>1990s</th>\n",
       "      <th>19th</th>\n",
       "      <th>abuse</th>\n",
       "      <th>accident</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>...</th>\n",
       "      <th>woods</th>\n",
       "      <th>word</th>\n",
       "      <th>worker</th>\n",
       "      <th>world</th>\n",
       "      <th>writer</th>\n",
       "      <th>written</th>\n",
       "      <th>year</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5025</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5029 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1950s  1960s  1970s  1980s  1990s  19th  abuse  accident  action  actor  \\\n",
       "0         0      0      0      0      0     0      0         0       0      0   \n",
       "1         0      0      0      0      0     0      0         0       0      0   \n",
       "2         0      0      0      0      0     0      0         0       0      0   \n",
       "3         0      0      0      0      0     0      0         0       0      0   \n",
       "4         0      0      0      0      0     0      0         0       0      0   \n",
       "...     ...    ...    ...    ...    ...   ...    ...       ...     ...    ...   \n",
       "5024      0      0      0      0      0     0      0         0       0      0   \n",
       "5025      0      0      0      0      0     0      0         0       0      0   \n",
       "5026      0      0      0      0      0     0      0         0       0      0   \n",
       "5027      0      0      0      0      0     0      0         0       0      0   \n",
       "5028      0      0      0      0      0     0      0         0       0      0   \n",
       "\n",
       "      ...  woods  word  worker  world  writer  written  year  york  young  \\\n",
       "0     ...      0     0       0      0       0        0     0     0      0   \n",
       "1     ...      0     0       0      0       0        0     0     0      0   \n",
       "2     ...      0     0       0      0       0        0     0     0      0   \n",
       "3     ...      0     0       0      0       0        0     0     0      0   \n",
       "4     ...      0     0       0      0       0        0     0     0      0   \n",
       "...   ...    ...   ...     ...    ...     ...      ...   ...   ...    ...   \n",
       "5024  ...      0     0       0      0       0        0     0     0      0   \n",
       "5025  ...      0     0       0      0       0        0     0     0      0   \n",
       "5026  ...      0     0       0      0       0        0     0     0      0   \n",
       "5027  ...      0     0       0      0       0        0     0     0      0   \n",
       "5028  ...      0     0       0      0       0        0     0     0      0   \n",
       "\n",
       "      zombie  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "5024       0  \n",
       "5025       0  \n",
       "5026       0  \n",
       "5027       0  \n",
       "5028       0  \n",
       "\n",
       "[5029 rows x 500 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "cv = CountVectorizer(max_features=500)\n",
    "pd.DataFrame(cv.fit_transform(df['plot_keywords']).todense(),\n",
    "columns=sorted(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5266ba4a-ca48-4720-918a-113b798b3cce",
   "metadata": {},
   "source": [
    "##### Task 3. Dealing with missing values\n",
    "\n",
    "We have discussed missing values a few times, and so far, we have just replaced them with some standard value such as zero and the empty string, but how do we correctly deal with missing values in data mining settings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b4d079-b25c-43e0-8983-2123368a64bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    760505847.0\n",
       "1    309404152.0\n",
       "2    200074175.0\n",
       "3    448130642.0\n",
       "4            NaN\n",
       "Name: gross, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gross'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d2817-3305-46fa-b15e-1dac27599287",
   "metadata": {},
   "source": [
    "First, it is important to make a few considerations:\n",
    "\n",
    "- Does it make sense to fill in the missing values? In the case of text, what are you going to fill it with? Is it easier to fix the problem or ignore the document altogether?\n",
    "\n",
    "- Are the missing values random? If it is just some small error, it will not affect your analysis if you delete the entries with missing values. Of course, that is supposing you have enough data.\n",
    "\n",
    "- If the missing values are not random, you can choose a few strategies to fix them without hurting the overall vector representation too much:\n",
    "\n",
    "    - Impute: replaces the missing values with some value inferred from the data (e.g., the mean/median feature value or the majority category).\n",
    "    - Estimate: replaces the missing values with some value learned from the data (e.g., using Singular Value Decomposition, k-NN, or Naive Bayes).\n",
    "    \n",
    "Estimation often proves effective when doing predictions afterward, even with up to half of the values missing. However, using parameterized models also introduces more complexity in your pipeline, so it is worthwhile to consider if time is a factor and if there are enough resources (in terms of data).\n",
    "\n",
    "#### Task 4. Imputing the missing values\n",
    "\n",
    "Doing this process column-wise in ``pandas`` requires a bit of effort to make it compatible with scikit-learn’s ``SimpleImputer``. Scikit-learn expects a matrix, so if we only want to impute one column, we need to wrap it between brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede42177-ce7f-4662-9ff6-ccdefcd751de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "df[['gross']] = imp.fit_transform(df[['gross']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca1ee3-6c32-42c9-bdae-3b85ed4d7b37",
   "metadata": {},
   "source": [
    "As we can see, it’s done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8c8ee2-0cf3-4551-8c95-82f10e2531bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7.605058e+08\n",
       "1    3.094042e+08\n",
       "2    2.000742e+08\n",
       "3    4.481306e+08\n",
       "4    4.854946e+07\n",
       "Name: gross, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gross'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb7550-c480-4a16-bf14-b58178b4bb53",
   "metadata": {},
   "source": [
    "A simpler alternative is using ``pandas`` native function for imputing missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b71714-eaef-432a-8cd6-742625db84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gross'] = df['gross'].fillna(df['gross'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c457f-0132-4784-9288-410f627c4e9b",
   "metadata": {},
   "source": [
    "##### Task 5. Dealing with discrete (categorical) data\n",
    "\n",
    "Unfortunately, there is no native way to deal with categorical data in the scikit-learn library. Notice that other machine learning libraries (e.g., Weka) do support the use of categorical data. If we want to use any scikit-learn, we need the entire dataframe to be numeric and remove all missing values. If we want to impute on most frequent categories, we have to count their occurrences, sort them by most frequent, take the top entry, and get its index value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb01352c-b675-49c8-afde-aac1157d4cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Drama                                       235\n",
       "Comedy                                      209\n",
       "Comedy|Drama                                189\n",
       "Comedy|Drama|Romance                        186\n",
       "Comedy|Romance                              158\n",
       "                                           ... \n",
       "Biography|Documentary|Drama                   1\n",
       "Action|Adventure|Animation|Comedy|Sci-Fi      1\n",
       "Action|Drama|Sport|Thriller                   1\n",
       "Crime|Documentary|Drama                       1\n",
       "Adventure|Sci-Fi                              1\n",
       "Name: genres, Length: 914, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['genres'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02c6a3b7-2789-447e-bebc-03b366f59de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drama'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['genres'].value_counts()[:1].index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6acec8-7f2d-463f-bac1-4da7667f0b15",
   "metadata": {},
   "source": [
    "To impute numerical and categorical features, we need a small piece of code. Pandas library has a ’category’ type that allows for easy conversion to numbers. We will use that in combination with the two other ``fillna`` lines that we discussed before to fill the missing values of all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "794a9673-fc3b-4cdc-9f27-d5ab4589a62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5029 entries, 0 to 5028\n",
      "Data columns (total 30 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   color                      5010 non-null   object \n",
      " 1   director_name              4926 non-null   object \n",
      " 2   num_critic_for_reviews     4981 non-null   float64\n",
      " 3   duration                   5014 non-null   float64\n",
      " 4   director_facebook_likes    4926 non-null   float64\n",
      " 5   actor_3_facebook_likes     5006 non-null   float64\n",
      " 6   actor_2_name               5016 non-null   object \n",
      " 7   actor_1_facebook_likes     5022 non-null   float64\n",
      " 8   gross                      5029 non-null   float64\n",
      " 9   genres                     5029 non-null   object \n",
      " 10  actor_1_name               5022 non-null   object \n",
      " 11  movie_title                5029 non-null   object \n",
      " 12  num_voted_users            5029 non-null   int64  \n",
      " 13  cast_total_facebook_likes  5029 non-null   int64  \n",
      " 14  actor_3_name               5006 non-null   object \n",
      " 15  facenumber_in_poster       5017 non-null   float64\n",
      " 16  plot_keywords              5029 non-null   object \n",
      " 17  movie_imdb_link            5029 non-null   object \n",
      " 18  num_user_for_reviews       5008 non-null   float64\n",
      " 19  language                   5017 non-null   object \n",
      " 20  country                    5024 non-null   object \n",
      " 21  content_rating             4732 non-null   object \n",
      " 22  budget                     4542 non-null   float64\n",
      " 23  title_year                 4922 non-null   float64\n",
      " 24  actor_2_facebook_likes     5016 non-null   float64\n",
      " 25  imdb_score                 5029 non-null   float64\n",
      " 26  aspect_ratio               4707 non-null   float64\n",
      " 27  movie_facebook_likes       5029 non-null   int64  \n",
      " 28  quality                    5029 non-null   int64  \n",
      " 29  imdb_z                     5029 non-null   float64\n",
      "dtypes: float64(14), int64(4), object(12)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info() # checking the types of columns and missing values \n",
    "# observe the object dtype and that the count of non-null (non-missing) is not the same for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7536a239-f7d7-44b7-8f02-6385d3aef331",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, dtype in df.dtypes.to_dict().items(): # for each column, dtype pair in the dataframe\n",
    "    \n",
    "    if dtype == 'object': # if the column is an object (thus discrete)\n",
    "        \n",
    "        df[column] = df[column].fillna(df[column].value_counts()[:1].index[0]) # fill with most common\n",
    "        cats = df[column].astype('category') # convert to category\n",
    "        df[column] = cats.cat.codes # use the category indices to convert to numeric\n",
    "        \n",
    "    else: # if the column is something else (thus numeric)\n",
    "        df[column] = df[column].fillna(df[column].mean()) # take the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "077cc648-454b-4b07-83ce-2ed200c5c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5029 entries, 0 to 5028\n",
      "Data columns (total 30 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   color                      5029 non-null   int8   \n",
      " 1   director_name              5029 non-null   int16  \n",
      " 2   num_critic_for_reviews     5029 non-null   float64\n",
      " 3   duration                   5029 non-null   float64\n",
      " 4   director_facebook_likes    5029 non-null   float64\n",
      " 5   actor_3_facebook_likes     5029 non-null   float64\n",
      " 6   actor_2_name               5029 non-null   int16  \n",
      " 7   actor_1_facebook_likes     5029 non-null   float64\n",
      " 8   gross                      5029 non-null   float64\n",
      " 9   genres                     5029 non-null   int16  \n",
      " 10  actor_1_name               5029 non-null   int16  \n",
      " 11  movie_title                5029 non-null   int16  \n",
      " 12  num_voted_users            5029 non-null   int64  \n",
      " 13  cast_total_facebook_likes  5029 non-null   int64  \n",
      " 14  actor_3_name               5029 non-null   int16  \n",
      " 15  facenumber_in_poster       5029 non-null   float64\n",
      " 16  plot_keywords              5029 non-null   int16  \n",
      " 17  movie_imdb_link            5029 non-null   int16  \n",
      " 18  num_user_for_reviews       5029 non-null   float64\n",
      " 19  language                   5029 non-null   int8   \n",
      " 20  country                    5029 non-null   int8   \n",
      " 21  content_rating             5029 non-null   int8   \n",
      " 22  budget                     5029 non-null   float64\n",
      " 23  title_year                 5029 non-null   float64\n",
      " 24  actor_2_facebook_likes     5029 non-null   float64\n",
      " 25  imdb_score                 5029 non-null   float64\n",
      " 26  aspect_ratio               5029 non-null   float64\n",
      " 27  movie_facebook_likes       5029 non-null   int64  \n",
      " 28  quality                    5029 non-null   int64  \n",
      " 29  imdb_z                     5029 non-null   float64\n",
      "dtypes: float64(14), int16(8), int64(4), int8(4)\n",
      "memory usage: 805.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info() # checking the changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3eed0b-2356-47dc-9502-ac41e00e1206",
   "metadata": {},
   "source": [
    "##### Task 6. Standardizing numerical features\n",
    "\n",
    "With all the missing values out of the way, we need to standardize our feature space. Most models benefit greatly from a Gaussian distribution, and therefore we can use scikit-learn’s StandardScaler to achieve this. While doing that, let us define the input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a939df8e-ff59-4eb9-aa73-0cbaabf00530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y = df.pop('quality') # returns and drops the column from the dataset\n",
    "X = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866780f-6a52-4857-8013-a39578369295",
   "metadata": {},
   "source": [
    "Are we finally ready to make predictions now? Not quite yet.\n",
    "\n",
    "##### Task 7. Splitting the available data into train and test\n",
    "\n",
    "Our dataframe is ready, and we can use it to build a classification model. However, before building our selected model, we first need to set up baselines and inspect their behavior. Let us start by at least making our train/test evaluation set-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5e5d44c-1a08-4727-89c8-f045dc6c0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d28092-0789-4ac4-99d5-6e7a61ea813e",
   "metadata": {},
   "source": [
    "#### Task 8. Building some baseline models for comparison purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424cdff6-596a-48e8-a93e-667978a678b1",
   "metadata": {},
   "source": [
    "Since we are not doing any parameter tuning, we do not require a validation set yet. Simple classification models such as Naive Bayes or Logistic Regression (without polynomial features, kernel transformations, or parameter tuning) make good first baselines.\n",
    "\n",
    "And, of course, we would also need a dummy majority baseline or zero-rule classifier. We can construct this simple model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f92d29b-0867-4869-ae2d-8dff969e371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline = [y.value_counts()[:1].index[0]] * len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9152bad8-7fce-4416-bc1c-76720a6f7718",
   "metadata": {},
   "source": [
    "This is the same ’most common class’ that we used for the imputation of categories. We then create a list of the length of the initial label, filled with this value. That is our majority baseline. We can quickly evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e003d368-3146-411d-b2d5-24cf08119b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        41\n",
      "           2       0.00      0.00      0.00       441\n",
      "           3       0.55      1.00      0.71      2774\n",
      "           4       0.00      0.00      0.00      1445\n",
      "           5       0.00      0.00      0.00       328\n",
      "\n",
      "    accuracy                           0.55      5029\n",
      "   macro avg       0.11      0.20      0.14      5029\n",
      "weighted avg       0.30      0.55      0.39      5029\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adama\\Anaconda2\\envs\\mypython36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\adama\\Anaconda2\\envs\\mypython36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\adama\\Anaconda2\\envs\\mypython36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, y_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c7d44b-bdf0-4e3a-9fd1-55551455fcae",
   "metadata": {},
   "source": [
    "#### Task 9. Building and evaluating the selected classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699024ac-5b33-4a6d-8132-481a4e6f9a05",
   "metadata": {},
   "source": [
    "Now for the selected classification model, let us use LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77973435-a272-40a9-b184-3cc151f9f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adama\\Anaconda2\\envs\\mypython36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e7c41-274d-47a0-8d1b-491f9932af42",
   "metadata": {},
   "source": [
    "We can evaluate what the model has learned as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e059b675-4995-4c97-b83c-82203e06810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.45      0.62        11\n",
      "           2       0.93      0.93      0.93       100\n",
      "           3       0.99      1.00      0.99       717\n",
      "           4       0.99      0.98      0.99       351\n",
      "           5       0.95      0.96      0.96        79\n",
      "\n",
      "    accuracy                           0.98      1258\n",
      "   macro avg       0.97      0.87      0.90      1258\n",
      "weighted avg       0.98      0.98      0.98      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71decad4-6d13-4f4f-b7e7-8cc7beb1a925",
   "metadata": {},
   "source": [
    "Well, aren’t we doing amazing? This is the point where some loud alarm bells need to start ringing. Why?\n",
    "\n",
    "- We determined that most of the features are useless; they do not accurately reflect the ’truth’ or contain mixed information.\n",
    "- The amount of information we have available is quite limited and would intuitively not be enough to guess movie quality this well (although, of course, this is speculation at this point).\n",
    "- We ran a vanilla model, no tuning, no nothing, and we are doing almost perfect on the test set.\n",
    "\n",
    "Let us see what the model is paying attention to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d904ef64-de77-4a34-9922-5b6a7b25bcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('color', -0.03066004905701737),\n",
       " ('director_name', 0.026970860139951768),\n",
       " ('num_critic_for_reviews', 0.13420271657544114),\n",
       " ('duration', 0.08215123980305461),\n",
       " ('director_facebook_likes', -0.0980873752415476),\n",
       " ('actor_3_facebook_likes', -0.09986619345211814),\n",
       " ('actor_2_name', 0.13982793492831092),\n",
       " ('actor_1_facebook_likes', -0.10832565921943706),\n",
       " ('gross', 0.06522750609433892),\n",
       " ('genres', -0.004179990908756446),\n",
       " ('actor_1_name', -0.05203343149257484),\n",
       " ('movie_title', -0.01119589722618288),\n",
       " ('num_voted_users', -0.20024115290581107),\n",
       " ('cast_total_facebook_likes', 0.33155800351316783),\n",
       " ('actor_3_name', -0.1846632220427074),\n",
       " ('facenumber_in_poster', 0.10024541236345444),\n",
       " ('plot_keywords', -0.08784301647318947),\n",
       " ('movie_imdb_link', -0.14805164570272275),\n",
       " ('num_user_for_reviews', -0.09943063504597484),\n",
       " ('language', 0.012595002030817782),\n",
       " ('country', 0.132291786264631),\n",
       " ('content_rating', 0.0949612858598149),\n",
       " ('budget', -0.051837469476016126),\n",
       " ('title_year', 0.32190009614492693),\n",
       " ('actor_2_facebook_likes', 0.024977827294643297),\n",
       " ('imdb_score', -1.7719751265219936),\n",
       " ('aspect_ratio', -0.03716172613723201),\n",
       " ('movie_facebook_likes', 0.017878298841840837)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(df.columns[:-1], lr.coef_[2])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10efeaa6-eb77-4655-9c76-7817e8b488b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1f357-f611-4ceb-9ac4-7826d0aa59b5",
   "metadata": {},
   "source": [
    "``LogisticRegression`` has coefficients per class, so this would be for class ’3’. Apparently 'imdb_score' is an amazing predictor for quality. Let’s see why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6076bd43-5bf1-42ae-a267-e99668a9ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = pd.concat([df['imdb_score'], y], axis=1).plot(kind='scatter', x='imdb_score', y='quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7becc1-09c8-4a61-8e9f-52753c2bad91",
   "metadata": {},
   "source": [
    "#### Task 10. Correcting the data and re-building the models\n",
    "\n",
    "- What is happening here?\n",
    "\n",
    "Hopefully, with this you will see that quality is actually based on imdb_score: 1 to 3 = 1, 3 to 5 = 2, 5 to 7 = 3, 7 to 8 = 4, 8 to 10 = 5. The model will see this and use this feature. Given that it is just a derivation from imdb_score, we do not want to use it, as it is a feature contamination/pollution case.\n",
    "\n",
    "- Try removing the polluting feature with ``del df['imdb_score']`` or ``df.drop('imdb_score', inplace=True, axis=1)`` and run the experiments again. You should get lower scores, but the coefficients should make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ad6c411-25a0-4bed-a619-4b52e70220f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.71      0.83         7\n",
      "           2       0.97      0.91      0.94       107\n",
      "           3       0.98      0.99      0.99       664\n",
      "           4       0.92      0.98      0.95       378\n",
      "           5       0.96      0.72      0.82       102\n",
      "\n",
      "    accuracy                           0.96      1258\n",
      "   macro avg       0.97      0.86      0.91      1258\n",
      "weighted avg       0.96      0.96      0.96      1258\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adama\\Anaconda2\\envs\\mypython36\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('color', -0.01857881925191206),\n",
       " ('director_name', 0.022551170939812726),\n",
       " ('num_critic_for_reviews', 0.368592399937743),\n",
       " ('duration', 0.16837668861346128),\n",
       " ('director_facebook_likes', -0.12942345220970555),\n",
       " ('actor_3_facebook_likes', -0.04549951365487586),\n",
       " ('actor_2_name', 0.02349647515062821),\n",
       " ('actor_1_facebook_likes', -0.005669297880670421),\n",
       " ('gross', 0.0784140181192109),\n",
       " ('genres', 0.011284743537481504),\n",
       " ('actor_1_name', -0.03836222301311909),\n",
       " ('movie_title', 0.07375055043032601),\n",
       " ('num_voted_users', -0.31841922947801204),\n",
       " ('cast_total_facebook_likes', 0.2991329616692036),\n",
       " ('actor_3_name', -0.12557352990200782),\n",
       " ('facenumber_in_poster', 0.07787744457260234),\n",
       " ('plot_keywords', -0.05580220449669781),\n",
       " ('movie_imdb_link', -0.18306920089786224),\n",
       " ('num_user_for_reviews', -0.23959120424226857),\n",
       " ('language', 0.033415672707136974),\n",
       " ('country', 0.12463328627350902),\n",
       " ('content_rating', 0.08307780070286908),\n",
       " ('budget', -0.1381310979070767),\n",
       " ('title_year', 0.2424218023605199),\n",
       " ('actor_2_facebook_likes', 0.015527639347436492),\n",
       " ('aspect_ratio', -0.05176741486271135),\n",
       " ('movie_facebook_likes', -0.04374448856517519)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('imdb_score', inplace=True, axis=1)\n",
    "\n",
    "X = scaler.fit_transform(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "list(zip(df.columns[:-1], lr.coef_[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578856a-8cf3-4584-974f-463060aa6223",
   "metadata": {},
   "source": [
    "##### Other performance measures\n",
    "\n",
    "You can also measure performance by calling individual functions. For more information, see the links below:\n",
    "\n",
    "*Accuracy*:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5314484f-3cf7-4d72-b089-9397140a66d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuray: 0.958664546899841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_pred, y_test)\n",
    "print('accuray: ' + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a788f00-2d10-4e98-8605-b2cb67c24845",
   "metadata": {},
   "source": [
    "Precision: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed181670-2916-401a-ba3d-f56a37d3c95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision: 0.9651263741249789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_pred, y_test, average='weighted')\n",
    "print('average precision: ' + str(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f97b2f-c170-4ae0-8782-aefb459af722",
   "metadata": {},
   "source": [
    "Recall: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "440a37f3-40b8-4b0c-ab41-23ebd9eae7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average recall: 0.958664546899841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_pred, y_test, average='weighted')\n",
    "print('average recall: ' + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f384f2-a31d-47ee-9df4-6a0327f02386",
   "metadata": {},
   "source": [
    "F1 measure: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ee77e3c-260b-43d9-93c8-ad54899d0ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average f1: 0.958664546899841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = recall_score(y_pred, y_test, average='weighted')\n",
    "print('average f1: ' + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048e8bc-140c-434c-b911-45d4928c8bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
